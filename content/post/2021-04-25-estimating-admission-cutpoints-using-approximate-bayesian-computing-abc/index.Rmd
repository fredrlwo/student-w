---
title: "Estimating Admission Cutpoints Using Approximate Bayesian Computing (ABC)"
author: "Fredrik Wollbraaten"
date: '2021-04-25'
slug: estimating-admission-cutpoints-using-approximate-bayesian-computing-abc
categories: blog
tags:
- Statistical Methods
- Applications
---

Each year, tens of thousands of Norwegian (and foreign) students apply for admission 
to college. Admission is mainly decided by the point score of the students applying to 
a particular programme, which is mainly made up by the students grades. As an example, if 1,500 apply to the law programme at the University of Oslo with 
230 places, then the top `r round(100*230/1500, 1)` % of the students in terms of points are admitted, and the admission cutpoint is the point score of this percentile. 

Taking another view, if we consider the population of potential students applying to the programme, 
the students with a point score above the $100\times(1500-230)/1500$ or `r round(100*(1500-230)/1500, 1)`-th percentile are admitted. After the application season is over, the admission cutpoints for each programme is published, in addition to the total number of applicants and available positions. 

The offers from the application round is available the 15th of July every year, and most college programmes commence about a month later, in mid August. This means that tens of thousands of new students have to move cross country, find housing and get settled in a mere 30 days, amidst other holiday plans. 

What if we could predict the admission cutpoints for all programmes earlier in the spring? In this post I will explore a simple model for the admission cutpoint, and use *Approximate Bayesian Computing* (ABC) to estimate the model parameters. 

# A model for the admission cutpoint

Given the three datapoints 

* *Admission cutpoint* $c_t$
* *Applicants* $a_t$
* *Available seats* $s_t$

for years $t = 1, ..., T$, how we can model the admission cutpoint given 
the number of applicants $a_t$ and available seats $s_t$ in a given year? Usually, thinking about the underlying process generating the data is a fruitful approach. 

In this case, we can assume that we have some population of potential applicants for a particular programme in year $t$. The point scores for these potential applicants follow some distribution parameterised by $\theta_t$. Then, the admission cutpoint is generated by drawing $a_t$ applicants from this distribution, and calculating $c_t = (a_t - s_t)/a_t$-quantile. In a Bayesian setting, we can then estimate the $\theta$-parameters by proposing a prior $p(\theta)$ and calculating the posterior distribution $$p(\theta|C) = \frac{p(C|\theta)p(\theta)}{p(C)}.$$

While the probabilistic process producing the admission cutpoints $c_{t}$ is simple, the likelihood of the parameters $\theta_t$ given the cutpoints $c_1, ..., c_T$ is non-standard. Often, we want to avoid formulating the likelihood if it is difficult to formulate or very computationally intense to calculate. The Covid-19 'R-number' model by FHI and UiO is one such example.

*Approximate Bayesian Computation (ABC)* is a method for estimating the posterior distribution in settings where the likelihood is difficult/impossible to formulate or evaluation is computationally intensive, while simulation is doable. Before we apply this method to the admission cutpoint data, I will briefly describe the main idea.

# Approximate Bayesian Computation

Assuming we have a parameter vector $\theta$, a prior $p(\theta)$, data $D$ and a likelihood $p(D|\theta)$ we can simulate from, we can approximate $p(\theta|D)$ by generating samples $\theta_i$ from it in the following fashion:

1. Simulate $\theta_i$ from $p(\theta)$.
2. Simulate $D_i$ from $p(D_i|\theta_i)$.
3. Accept $\theta_i$ if $\rho(D, D_i)\leq \epsilon$.

In other words, $\theta_i$ is considered a parameter vector sampled from the posterior if the corresponding generated dataset $D_i$ is close to $D$ in some sense. The method is exact if $\epsilon = 0$.

A proof for this is simple in a setting where $p(D) > 0$. Let $\mathbb{I}_{D_i = D}$, and consider the joint distribution of $\theta_i$ and $\mathbb{I}_{D_i = D}$ generated by the algorithm: 
$$p(\theta_i)\times p(D_i = D |\theta_i)^{\mathbb{I}_{D_i = D}}p(D_i \neq D|\theta_i)^{\mathbb{I}_{D_i \neq D}}.$$ It is clear that the distribution of $\theta_i | \mathbb{I}_{D_i = D} = 1$ is proportional to $p(\theta_i)\times p(D_i = D|\theta_i)$.

## Choice of discrepancy measure $\rho$

The main headache in the basic ABC algorithm is the choice of discrepancy measure $\rho$ and threshold $\epsilon$. The most naive $\rho$ is a distance measure between the actual datasets $D_i$ and $D$, but the curse of dimensionality quickly ensures that the probability of accepting $\theta_i$ approaches 0. 

Usually, the discrepancy measure is reduced to measuring the difference between summary statistics, e.g $\theta_i$ is accepted if $\rho(S(D_i), S(D)) \leq \epsilon$. If a sufficient summary statistic is available for a particular setting, the ABC algorithm is exact when $\epsilon$ approaches 0. 

In general, one must accept some information loss when using summary statistics, but the approximation of the posterior distribution is usually acceptable. 

In our setting, a particular choice of summary statistic is very natural. Our observed data is actually just the empirical $100 \times (a_t - s_t)/a_t$-percentile of the population of applicants. Thus, a natural discrepancy measure is the squared distance between the observed empirical $100 \times (a_t - s_t)/a_t$-percentile and the corresponding generated percentile. 

# Estimating the admission cutpoint

Back to the question on estimating programme admission cutpoints. We need to choose a distribution for the point scores of the applicants. A standard assumption is that students' grades are bell shaped. Since the point scores is a sum of many 'bell-shaped' grades, it is not unreasonable to assume that the inherently discrete point scores are approximately normal. 

However, the choice of normally distributed point scores introduces an estimation problem. If we believe that the mean and variance of the point score distribution of applicants is different for different years, we have an identifiability problem. We are trying to estimate two parameters based on observing a single empirical quantile of the distribution. Since quantiles in the normal distribution is a function of both the mean and the variance, there is an infinite number of $(\mu_t, \sigma_t^2)$-parameters corresponding to any quantile. 

The solution is to use prior information. The first piece of prior information is that the mean point score of potential applicants is slowly varying from year to year, i.e we are smoothing the $\mu_t$-estimates. This is done by assuming a random walk on $\mu_t$:
$$p(\mu_1) = \mathcal{N}(40, 5^2) \text{ and } p(\mu_{t+1}|\mu_t) = \mathcal{N}(\mu_t, \delta^2).$$ The second piece of prior information is that the variance $\sigma^2$ is equal for all years, with a prior distribution $p(\sigma)\sim\text{Gamma}(\alpha, \beta)$.

Finally, we need to choose $\delta$ and $(\alpha, \beta)$. To choose $\delta$ we can take a look at the change of the average grade in high school between 2009 and 2020. Very roughly, the average grade has changed 0.5 over these 11 years, meaning that the point score has changed on the order of $10\times 0.5 = 5$ points. Since we only have aggregate data, we set $\delta$ by selecting the one which maximized the probability of seeing a change of 5 points in a normal density with mean 0 and standard deviation $\sqrt{11}\delta$. We maximize the log likelihood by setting the derivative equal to 0:
$$-\frac{1}{2} \frac{d}{d\delta} \left(\log(2\pi 11\delta^2) + \frac{5^2}{11\delta^2}\right)=\frac{2(\delta^2 - 5^2)}{\delta^3}.$$ The $\delta$ which maximizes this expression is $\delta = \frac{5}{\sqrt{11}}\approx 1.5$


```{r, message=FALSE, warning=FALSE}
require(tidyverse)
law <- read_csv("law.csv") %>% rename(c_t = Poenggrense,
                                      a_t = SÃ¸kere,
                                      s_t = Plasser) %>% mutate(y_t = 2009:2020)
ggplot(law, aes(x = y_t, y = c_t)) +
  geom_line()

objective <- function(c, mu, sigma, a, s){
  abs(s - a*(1-pnorm(q = c, mean = mu, sd = sigma)))
}

prior_mu_1_mu <- 40 #Prior mean for the average point score in 2009
prior_mu_1_sig <- 5 #Prior std.dev for the average point score in 2009
delta_mu_sig <- 5/sqrt(11) #Std.dev of the mu_t+1 - mu_t random walk prior

sig_alpha <- 10
sig_beta <- 1 #scale 

iterations <- 1e4
eps <- 3

a_t <- law %>% select(a_t) %>% pull()
s_t <- law %>% select(s_t) %>% pull()
c_t <- law %>% select(c_t) %>% pull()
q_t <- (a_t - s_t)/a_t

n_yrs <- 12
mu <- rep(0, n_yrs)
sim_c_t <- mu
sigma <- 0

acc_mu <- matrix(0, nrow = iterations, ncol = n_yrs)
acc_sigma <- rep(0, iterations)

for(i in 1:iterations){
  mu[1] <- rnorm(1, prior_mu_1_mu, prior_mu_1_sig)
  mu[2:n_yrs] <- mu[1] + cumsum(rnorm(n_yrs - 1, 0, delta_mu_sig))
  sigma <- rgamma(1, shape = sig_alpha, scale = sig_beta)
  for(t in 1:n_yrs){
    y_t <- rnorm(a_t[t], mu[t], sigma)
    sim_c_t[t] <- quantile(y_t, q_t[t])
  }
  
  rho <- mean((sim_c_t - c_t)^2)
  if(rho < eps){
    acc_mu[i, ] <- mu
    acc_sigma[i] <- sigma
  }
}

acc_mu <- acc_mu[rowMeans(acc_mu) != 0, ] 
acc_sigma <- acc_sigma[acc_sigma > 0]

post_mu <- colMeans(acc_mu, na.rm = TRUE)
post_sigma <- mean(acc_sigma, na.rm = TRUE)
est_admission <- post_mu*0
for(t in 1:length(post_mu)){
  est_c_t = optim(50, objective, a = a_t[t], s = s_t[t], mu = post_mu[t], sigma = post_sigma)
  est_admission[t] <- est_c_t$par
}

result <- bind_rows(tibble(Year = 2009:2020, c_t = c_t, type = "Actual"),
                    tibble(Year = 2009:2020, c_t = est_admission, type = "Estimated"))

ggplot(result, aes(x = Year, y = c_t)) +
  geom_line(aes(linetype = type))
```

Now, when we have a model which adequately describes the observed data, now to the actual useful part. Let's predict the admission cut-off for the law programme in 2021, for which we only need the number of applicants and seats:

*$a = 1929$ *applicants*.
*$s = 234$ *available seats*.

We need to estimate $C_{21}|c_{09:20}, a_{09:20}, s_{09:20}$. This can be done by simulation as usual, through the identity
$$\mathrm{E}[c_{21}|c_{09:20}, a_{09:21}, s_{09:21}] = \int_{c_{21}} c_{21}p(c_{21}|c_{09:20}, a_{09:21}, s_{09:21})d c_{21} = \int_{c_{21}}c_{21}p(c_{21}|\mu_{21}, \sigma, a_{21}, s_{21})p(\mu_{09:21}, \sigma|c_{09:20}, a_{09:21}, s_{09:21})d c_{21}$$ with $\theta = (\mu_{2021}, \sigma).$ Note that $$p(\mu_{09:21}, \sigma|c_{09:20}, a_{09:21}, s_{09:21}) = p(\mu_{21}|\mu_{20})p(\mu_{09:20}, \sigma|c_{09:20}, a_{09:20}, s_{09:20}).$$ Further, we already have samples from $$p(\mu_{09:20}, \sigma|c_{09:20}, a_{09:20}, s_{09:20}).$$ Thus, to estimate $p(c_{21}|c_{09:20}, a_{09:20}, s_{09:20})$ we can draw a sample from our set of posterior samples, then draw $\mu_{21}$ from $p(\mu_{21}|\mu_{20})$ (hyperparameters omitted), and then we can calculate $$c_{21}|\mu_{21}, \sigma, a_{21}, s_{21}.$$

```{r}
a_21 <- 1929
s_21 <- 234
q_21 <- (a_21 - s_21)/a_21

iterations <- 1e4
sim_c_21 <- rep(0, iterations)
ids <- sample(nrow(acc_mu), iterations, replace = T)

for(t in 1:iterations){
  mu_21 <- rnorm(1, acc_mu[ids[t], ncol(acc_mu)], delta_mu_sig)
  sigma <- acc_sigma[ids[t]]
  y_21 <- rnorm(a_21, mu_21, sigma)
  sim_c_21[t] <- quantile(y_21, q_21)
}

predicted <- tibble(Year = 2021, c_t = mean(sim_c_21), sigma = sd(sim_c_21))
ggplot(result, aes(x = Year, y = c_t)) +
  geom_line(aes(linetype = type)) +
  geom_point(data = predicted, aes(x = Year, y = c_t)) +
  geom_line(data = tibble(Year = 2009:2020, mu = post_mu), aes(x = Year, y = mu)) +
  geom_errorbar(data = predicted, aes(x = Year, ymin = c_t - 0.67*sigma, ymax = c_t + 0.67*sigma))
```

