---
title: "Estimating Admission Cutpoints Using Approximate Bayesian Computing (ABC)"
author: "Fredrik Wollbraaten"
date: '2021-04-25'
slug: estimating-admission-cutpoints-using-approximate-bayesian-computing-abc
categories: blog
tags:
- Statistical Methods
- Applications
---

Each year, tens of thousands of Norwegian (and foreign) students apply for admission 
to college. Admission is mainly decided by the point score of the students applying to 
a particular programme, which is mainly made up by the students grades. As an example, if 1,500 apply to the law programme at the University of Oslo with 
230 places, then the top `r round(100*230/1500, 1)` % of the students in terms of points are admitted, and the admission cutpoint is the point score of this percentile. 

Taking another view, if we consider the population of potential students applying to the programme, 
the students with a point score above the $100\times(1500-230)/1500$ or `r round(100*(1500-230)/1500, 1)`-th percentile are admitted. After the application season is over, the admission cutpoints for each programme is published, in addition to the total number of applicants and available positions. 

The offers from the application round is available the 15th of July every year, and most college programmes commence about a month later, in mid August. This means that tens of thousands of new students have to move cross country, find housing and get settled in a mere 30 days, amidst other holiday plans. 

What if we could predict the admission cutpoints for all programmes earlier in the spring? In this post I will explore a simple model for the admission cutpoint, and use *Approximate Bayesian Computing* (ABC) to estimate the model parameters. 

# A model for the admission cutpoint

Given the three datapoints 

* *Admission cutpoint* $c_t$
* *Applicants* $a_t$
* *Available seats* $s_t$

for years $t = 1, ..., T$, how we can model the admission cutpoint given 
the number of applicants $a_t$ and available seats $s_t$ in a given year? Usually, thinking about the underlying process generating the data is a fruitful approach. 

In this case, we can assume that we have some population of applicants for a particular programme in year $t$, parameterised by $\theta_t$. Then, the admission cutpoint is generated by drawing $a_t$ applicants from this distribution, and calculating $c_t = (a_t - s_t)/a_t$-quantile. In a Bayesian setting, we can then estimate the $\theta$-parameters by proposing a prior $p(\theta)$ and calculating the posterior distribution $$p(\theta|C) = \frac{p(C|\theta)p(\theta)}{p(C)}.$$

While the probabilistic process producing the admission cutpoints $C$ is simple, actually formulating the likelihood of the parameters $\theta_t$ given the cutpoints $c_1, ..., c_T$ is rather difficult (at least for me, it seems like this needs marginalizing out all individual applicants scores...)

However, *Approximate Bayesian Computation (ABC)* is a method for estimating the posterior distribution in settings where the likelihood is difficult/impossible to formulate or evaluation is computationally intensive, while simulation is doable. Before we apply this method to the admission cutpoint data, I will briefly describe the main idea.

# Approximate Bayesian Computation

Assuming we have a parameter vector $\theta$, a prior $p(\theta)$, data $D$ and a likelihood $p(D|\theta)$ we can simulate from, we can approximate $p(\theta|D)$ by generating samples $\theta_i$ from it in the following fashion:

1. Simulate $\theta_i$ from $p(\theta)$.
2. Simulate $D_i$ from $p(D_i|\theta_i)$.
3. Accept $\theta_i$ if $\rho(D, D_i)\leq \epsilon$.

In other words, $\theta_i$ is considered a parameter vector sampled from the posterior if the corresponding generated dataset $D_i$ is close to $D$ in some sense. The method is exact if $\epsilon = 0$.

A proof for this is simple in a setting where $p(D) > 0$. Let $\mathbb{I}_{D_i = D}$, and consider the joint distribution of $\theta_i$ and $\mathbb{I}_{D_i = D}$ generated by the algorithm: 
$$p(\theta_i)\times p(D_i = D |\theta_i)^{\mathbb{I}_{D_i = D}}p(D_i \neq D|\theta_i)^{\mathbb{I}_{D_i \neq D}}.$$ It is clear that the distribution of $\theta_i | \mathbb{I}_{D_i = D} = 1$ is proportional to $p(\theta_i)\times p(D_i = D|\theta_i)$.

## Choice of discrepancy measure $\rho$

The main headache in the basic ABC algorithm is the choice of discrepancy measure $\rho$ and threshold $\epsilon$. The most naive $\rho$ is a distance measure between the actual datasets $D_i$ and $D$, but the curse of dimensionality quickly ensures that the probability of accepting $\theta_i$ approaches 0. 

Usually, the discrepancy measure is reduced to measuring the difference between summary statistics, e.g $\theta_i$ is accepted if $\rho(S(D_i), S(D)) \leq \epsilon$. If a sufficient summary statistic is available for a particular setting, the ABC algorithm is exact when $\epsilon$ approaches 0. 

In general, one must accept some information loss when using summary statistics, but the approximation of the posterior distribution is usually acceptable. 

In our setting, a particular choice of summary statistic is very natural. Our observed data is actually just the empirical $100 \times (a_t - s_t)/a_t$-percentile of the population of applicants. Thus, a natural discrepancy measure is the squared distance between the observed empirical $100 \times (a_t - s_t)/a_t$-percentile and the corresponding generated percentile. 

# Estimating the admission cutpoint

Back to the question on estimating programme admission cutpoints, we need to choose a distribution for the point scores of the applicants. Often, students' grades are approximately normally distributed in a discrete setting. Further, the point scores is a sum of many independent grades, which should also then be approximately normal. 

However, the choice of normally distributed point scores introduces another estimation problem. If we believe that the mean and variance is slowly changing between consecutive years, two parameters must be estimated by observing only one empirical percentile of the distribution. Since percentiles in the normal distribution is a function of both the mean and the variance, there is an infinite combination of $(\mu_t, \sigma_t^2)$-parameters corresponding to one particular percentile. 

The solution is to use prior information. For example, we can assume that the variance is approximately equal for all years. This might be reasonable, and this will probably lead to identifiable $\mu_t$-parameters. 

```{r, message=FALSE}
require(tidyverse)
law <- read_csv("law.csv") %>% rename(c_t = Poenggrense,
                                      a_t = SÃ¸kere,
                                      s_t = Plasser) %>% mutate(y_t = 2009:2020)
ggplot(law, aes(x = y_t, y = c_t)) +
  geom_line()
 
prior_mu_1_mu <- 40 #Prior mean for the average point score in 2009
prior_mu_1_sig <- 5 #Prior std.dev for the average point score in 2009
delta_mu_sig <- 1 #Std.dev of the mu_t+1 - mu_t random walk prior

sig_alpha <- 5
sig_beta <- 1 #scale 

iterations <- 1e4
eps <- 10

n_yrs <- 12
a_t <- law %>% select(a_t) %>% pull()
s_t <- law %>% select(s_t) %>% pull()
c_t <- law %>% select(c_t) %>% pull()
q_t <- (a_t - s_t)/a_t

mu <- rep(0, 11)
sim_c_t <- mu
sigma <- 0

acc_mu <- matrix(NA, nrow = 1, ncol = n_yrs)
acc_sigma <- c(NA)

for(i in 1:iterations){
  mu[1] <- rnorm(1, prior_mu_1_mu, prior_mu_1_sig)
  mu[2:n_yrs] <- mu[1] + cumsum(rnorm(n_yrs - 1, 0, delta_mu_sig))
  sigma <- rgamma(1, shape = 5, scale = 1)
  for(t in 1:n_yrs){
    y_t <- rnorm(a_t[t], mu[t], sigma)
    sim_c_t[t] <- quantile(y_t, q_t[t])
  }
  
  rho <- mean((sim_c_t - c_t)^2)
  if(rho < eps){
    acc_mu <- rbind(acc_mu, mu)
    acc_sigma <- c(acc_sigma, sigma)
  }
}

post_mu <- colMeans(acc_mu, na.rm = TRUE)
post_sigma <- mean(acc_sigma, na.rm = TRUE)

plot(2009:2020, post_mu)
```
