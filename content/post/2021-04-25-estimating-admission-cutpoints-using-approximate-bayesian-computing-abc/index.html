---
title: "Estimating Admission Cutpoints Using Approximate Bayesian Computing (ABC)"
author: "Fredrik Wollbraaten"
date: '2021-04-25'
slug: estimating-admission-cutpoints-using-approximate-bayesian-computing-abc
categories: blog
tags:
- Statistical Methods
- Applications
---



<p>Each year, tens of thousands of Norwegian (and foreign) students apply for admission
to college. Admission is mainly decided by the point score of the students applying to
a particular programme, which is mainly made up by the students grades. As an example, if 1,500 apply to the law programme at the University of Oslo with
230 places, then the top 15.3 % of the students in terms of points are admitted, and the admission cutpoint is the point score of this percentile.</p>
<p>Taking another view, if we consider the population of potential students applying to the programme,
the students with a point score above the <span class="math inline">\(100\times(1500-230)/1500\)</span> or 84.7-th percentile are admitted. After the application season is over, the admission cutpoints for each programme is published, in addition to the total number of applicants and available positions.</p>
<p>The offers from the application round is available the 15th of July every year, and most college programmes commence about a month later, in mid August. This means that tens of thousands of new students have to move cross country, find housing and get settled in a mere 30 days, amidst other holiday plans.</p>
<p>What if we could predict the admission cutpoints for all programmes earlier in the spring? In this post I will explore a simple model for the admission cutpoint, and use <em>Approximate Bayesian Computing</em> (ABC) to estimate the model parameters.</p>
<div id="a-model-for-the-admission-cutpoint" class="section level1">
<h1>A model for the admission cutpoint</h1>
<p>Given the three datapoints</p>
<ul>
<li><em>Admission cutpoint</em> <span class="math inline">\(c_t\)</span></li>
<li><em>Applicants</em> <span class="math inline">\(a_t\)</span></li>
<li><em>Available seats</em> <span class="math inline">\(s_t\)</span></li>
</ul>
<p>for years <span class="math inline">\(t = 1, ..., T\)</span>, how we can model the admission cutpoint given
the number of applicants <span class="math inline">\(a_t\)</span> and available seats <span class="math inline">\(s_t\)</span> in a given year? Usually, thinking about the underlying process generating the data is a fruitful approach.</p>
<p>In this case, we can assume that we have some population of potential applicants for a particular programme in year <span class="math inline">\(t\)</span>. The point scores for these potential applicants follow some distribution parameterised by <span class="math inline">\(\theta_t\)</span>. Then, the admission cutpoint is generated by drawing <span class="math inline">\(a_t\)</span> applicants from this distribution, and calculating <span class="math inline">\(c_t = (a_t - s_t)/a_t\)</span>-quantile. In a Bayesian setting, we can then estimate the <span class="math inline">\(\theta\)</span>-parameters by proposing a prior <span class="math inline">\(p(\theta)\)</span> and calculating the posterior distribution <span class="math display">\[p(\theta|C) = \frac{p(C|\theta)p(\theta)}{p(C)}.\]</span></p>
<p>While the probabilistic process producing the admission cutpoints <span class="math inline">\(c_{t}\)</span> is simple, the likelihood of the parameters <span class="math inline">\(\theta_t\)</span> given the cutpoints <span class="math inline">\(c_1, ..., c_T\)</span> is non-standard. Often, we want to avoid formulating the likelihood if it is difficult to formulate or very computationally intense to calculate. The Covid-19 ‘R-number’ model by FHI and UiO is one such example.</p>
<p><em>Approximate Bayesian Computation (ABC)</em> is a method for estimating the posterior distribution in settings where the likelihood is difficult/impossible to formulate or evaluation is computationally intensive, while simulation is doable. Before we apply this method to the admission cutpoint data, I will briefly describe the main idea.</p>
</div>
<div id="approximate-bayesian-computation" class="section level1">
<h1>Approximate Bayesian Computation</h1>
<p>Assuming we have a parameter vector <span class="math inline">\(\theta\)</span>, a prior <span class="math inline">\(p(\theta)\)</span>, data <span class="math inline">\(D\)</span> and a likelihood <span class="math inline">\(p(D|\theta)\)</span> we can simulate from, we can approximate <span class="math inline">\(p(\theta|D)\)</span> by generating samples <span class="math inline">\(\theta_i\)</span> from it in the following fashion:</p>
<ol style="list-style-type: decimal">
<li>Simulate <span class="math inline">\(\theta_i\)</span> from <span class="math inline">\(p(\theta)\)</span>.</li>
<li>Simulate <span class="math inline">\(D_i\)</span> from <span class="math inline">\(p(D_i|\theta_i)\)</span>.</li>
<li>Accept <span class="math inline">\(\theta_i\)</span> if <span class="math inline">\(\rho(D, D_i)\leq \epsilon\)</span>.</li>
</ol>
<p>In other words, <span class="math inline">\(\theta_i\)</span> is considered a parameter vector sampled from the posterior if the corresponding generated dataset <span class="math inline">\(D_i\)</span> is close to <span class="math inline">\(D\)</span> in some sense. The method is exact if <span class="math inline">\(\epsilon = 0\)</span>.</p>
<p>A proof for this is simple in a setting where <span class="math inline">\(p(D) &gt; 0\)</span>. Let <span class="math inline">\(\mathbb{I}_{D_i = D}\)</span>, and consider the joint distribution of <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(\mathbb{I}_{D_i = D}\)</span> generated by the algorithm:
<span class="math display">\[p(\theta_i)\times p(D_i = D |\theta_i)^{\mathbb{I}_{D_i = D}}p(D_i \neq D|\theta_i)^{\mathbb{I}_{D_i \neq D}}.\]</span> It is clear that the distribution of <span class="math inline">\(\theta_i | \mathbb{I}_{D_i = D} = 1\)</span> is proportional to <span class="math inline">\(p(\theta_i)\times p(D_i = D|\theta_i)\)</span>.</p>
<div id="choice-of-discrepancy-measure-rho" class="section level2">
<h2>Choice of discrepancy measure <span class="math inline">\(\rho\)</span></h2>
<p>The main headache in the basic ABC algorithm is the choice of discrepancy measure <span class="math inline">\(\rho\)</span> and threshold <span class="math inline">\(\epsilon\)</span>. The most naive <span class="math inline">\(\rho\)</span> is a distance measure between the actual datasets <span class="math inline">\(D_i\)</span> and <span class="math inline">\(D\)</span>, but the curse of dimensionality quickly ensures that the probability of accepting <span class="math inline">\(\theta_i\)</span> approaches 0.</p>
<p>Usually, the discrepancy measure is reduced to measuring the difference between summary statistics, e.g <span class="math inline">\(\theta_i\)</span> is accepted if <span class="math inline">\(\rho(S(D_i), S(D)) \leq \epsilon\)</span>. If a sufficient summary statistic is available for a particular setting, the ABC algorithm is exact when <span class="math inline">\(\epsilon\)</span> approaches 0.</p>
<p>In general, one must accept some information loss when using summary statistics, but the approximation of the posterior distribution is usually acceptable.</p>
<p>In our setting, a particular choice of summary statistic is very natural. Our observed data is actually just the empirical <span class="math inline">\(100 \times (a_t - s_t)/a_t\)</span>-percentile of the population of applicants. Thus, a natural discrepancy measure is the squared distance between the observed empirical <span class="math inline">\(100 \times (a_t - s_t)/a_t\)</span>-percentile and the corresponding generated percentile.</p>
</div>
</div>
<div id="estimating-the-admission-cutpoint" class="section level1">
<h1>Estimating the admission cutpoint</h1>
<p>Back to the question on estimating programme admission cutpoints. We need to choose a distribution for the point scores of the applicants. A standard assumption is that students’ grades are bell shaped. Since the point scores is a sum of many ‘bell-shaped’ grades, it is not unreasonable to assume that the inherently discrete point scores are approximately normal.</p>
<p>However, the choice of normally distributed point scores introduces an estimation problem. If we believe that the mean and variance of the point score distribution of applicants is different for different years, we have an identifiability problem. We are trying to estimate two parameters based on observing a single empirical quantile of the distribution. Since quantiles in the normal distribution is a function of both the mean and the variance, there is an infinite number of <span class="math inline">\((\mu_t, \sigma_t^2)\)</span>-parameters corresponding to any quantile.</p>
<p>The solution is to use prior information. The first piece of prior information is that the mean point score of potential applicants is slowly varying from year to year, i.e we are smoothing the <span class="math inline">\(\mu_t\)</span>-estimates. This is done by assuming a random walk on <span class="math inline">\(\mu_t\)</span>:
<span class="math display">\[p(\mu_1) = \mathcal{N}(40, 5^2) \text{ and } p(\mu_{t+1}|\mu_t) = \mathcal{N}(\mu_t, \delta^2).\]</span> The second piece of prior information is that the variance <span class="math inline">\(\sigma^2\)</span> is equal for all years, with a prior distribution <span class="math inline">\(p(\sigma)\sim\text{Gamma}(\alpha, \beta)\)</span>.</p>
<p>Finally, we need to choose <span class="math inline">\(\delta\)</span> and <span class="math inline">\((\alpha, \beta)\)</span>. To choose <span class="math inline">\(\delta\)</span> we can take a look at the change of the average grade in high school between 2009 and 2020.</p>
<pre class="r"><code>require(tidyverse)
law &lt;- read_csv(&quot;law.csv&quot;) %&gt;% rename(c_t = Poenggrense,
                                      a_t = Søkere,
                                      s_t = Plasser) %&gt;% mutate(y_t = 2009:2020)
ggplot(law, aes(x = y_t, y = c_t)) +
  geom_line()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>objective &lt;- function(c, mu, sigma, a, s){
  abs(s - a*(1-pnorm(q = c, mean = mu, sd = sigma)))
}

prior_mu_1_mu &lt;- 40 #Prior mean for the average point score in 2009
prior_mu_1_sig &lt;- 5 #Prior std.dev for the average point score in 2009
delta_mu_sig &lt;- 2 #Std.dev of the mu_t+1 - mu_t random walk prior

sig_alpha &lt;- 10
sig_beta &lt;- 1 #scale 

iterations &lt;- 1e3
eps &lt;- 3

n_yrs &lt;- 12
a_t &lt;- law %&gt;% select(a_t) %&gt;% pull()
s_t &lt;- law %&gt;% select(s_t) %&gt;% pull()
c_t &lt;- law %&gt;% select(c_t) %&gt;% pull()
q_t &lt;- (a_t - s_t)/a_t

mu &lt;- rep(0, 11)
sim_c_t &lt;- mu
sigma &lt;- 0

acc_mu &lt;- matrix(0, nrow = iterations, ncol = n_yrs)
acc_sigma &lt;- rep(0, iterations)

for(i in 1:iterations){
  mu[1] &lt;- rnorm(1, prior_mu_1_mu, prior_mu_1_sig)
  mu[2:n_yrs] &lt;- mu[1] + cumsum(rnorm(n_yrs - 1, 0, delta_mu_sig))
  sigma &lt;- rgamma(1, shape = sig_alpha, scale = sig_beta)
  for(t in 1:n_yrs){
    y_t &lt;- rnorm(a_t[t], mu[t], sigma)
    sim_c_t[t] &lt;- quantile(y_t, q_t[t])
  }
  
  rho &lt;- mean((sim_c_t - c_t)^2)
  if(rho &lt; eps){
    acc_mu[i, ] &lt;- mu
    acc_sigma[i] &lt;- sigma
  }
}

acc_mu &lt;- acc_mu[rowMeans(acc_mu) != 0, ] 
acc_sigma &lt;- acc_sigma[acc_sigma &gt; 0]

post_mu &lt;- colMeans(acc_mu, na.rm = TRUE)
post_sigma &lt;- mean(acc_sigma, na.rm = TRUE)
est_admission &lt;- post_mu*0
for(t in 1:length(post_mu)){
  est_c_t = optim(50, objective, a = a_t[t], s = s_t[t], mu = post_mu[t], sigma = post_sigma)
  est_admission[t] &lt;- est_c_t$par
}

result &lt;- bind_rows(tibble(Year = 2009:2020, c_t = c_t, type = &quot;Actual&quot;),
                    tibble(Year = 2009:2020, c_t = est_admission, type = &quot;Estimated&quot;))

ggplot(result, aes(x = Year, y = c_t)) +
  geom_line(aes(linetype = type))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<p>Now, when we have a model which adequately describes the observed data, now to the actual useful part. Let’s predict the admission cut-off for the law programme in 2021, for which we only need the number of applicants and seats:</p>
<p><em><span class="math inline">\(a = 1929\)</span> </em>applicants<em>.
</em><span class="math inline">\(s = 234\)</span> <em>available seats</em>.</p>
<p>We need to estimate <span class="math inline">\(C_{21}|c_{09:20}, a_{09:20}, s_{09:20}\)</span>. This can be done by simulation as usual, through the identity
<span class="math display">\[\mathrm{E}[c_{21}|c_{09:20}, a_{09:21}, s_{09:21}] = \int_{c_{21}} c_{21}p(c_{21}|c_{09:20}, a_{09:21}, s_{09:21})d c_{21} = \int_{c_{21}}c_{21}p(c_{21}|\mu_{21}, \sigma, a_{21}, s_{21})p(\mu_{09:21}, \sigma|c_{09:20}, a_{09:21}, s_{09:21})d c_{21}\]</span> with <span class="math inline">\(\theta = (\mu_{2021}, \sigma).\)</span> Note that <span class="math display">\[p(\mu_{09:21}, \sigma|c_{09:20}, a_{09:21}, s_{09:21}) = p(\mu_{21}|\mu_{20})p(\mu_{09:20}, \sigma|c_{09:20}, a_{09:20}, s_{09:20}).\]</span> Further, we already have samples from <span class="math display">\[p(\mu_{09:20}, \sigma|c_{09:20}, a_{09:20}, s_{09:20}).\]</span> Thus, to estimate <span class="math inline">\(p(c_{21}|c_{09:20}, a_{09:20}, s_{09:20})\)</span> we can draw a sample from our set of posterior samples, then draw <span class="math inline">\(\mu_{21}\)</span> from <span class="math inline">\(p(\mu_{21}|\mu_{20})\)</span> (hyperparameters omitted), and then we can calculate <span class="math display">\[c_{21}|\mu_{21}, \sigma, a_{21}, s_{21}.\]</span></p>
<pre class="r"><code>a_21 &lt;- 1929
s_21 &lt;- 234
q_21 &lt;- (a_21 - s_21)/a_21

iterations &lt;- 1e4
sim_c_21 &lt;- rep(0, iterations)
ids &lt;- sample(nrow(acc_mu), iterations, replace = T)

for(t in 1:iterations){
  mu_21 &lt;- rnorm(1, acc_mu[ids[t], ncol(acc_mu)], delta_mu_sig)
  sigma &lt;- acc_sigma[ids[t]]
  y_21 &lt;- rnorm(a_21, mu_21, sigma)
  sim_c_21[t] &lt;- quantile(y_21, q_21)
}

predicted &lt;- tibble(Year = 2021, c_t = mean(sim_c_21), sigma = sd(sim_c_21))
ggplot(result, aes(x = Year, y = c_t)) +
  geom_line(aes(linetype = type)) +
  geom_point(data = predicted, aes(x = Year, y = c_t)) +
  geom_line(data = tibble(Year = 2009:2020, mu = post_mu), aes(x = Year, y = mu)) +
  geom_errorbar(data = predicted, aes(x = Year, ymin = c_t - 0.67*sigma, ymax = c_t + 0.67*sigma))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
