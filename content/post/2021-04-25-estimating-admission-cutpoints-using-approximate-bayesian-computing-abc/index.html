---
title: "Estimating Admission Cutpoints Using Approximate Bayesian Computing (ABC)"
author: "Fredrik Wollbraaten"
date: '2021-04-25'
slug: estimating-admission-cutpoints-using-approximate-bayesian-computing-abc
categories: blog
tags:
- Statistical Methods
- Applications
---



<p>Each year, tens of thousands of Norwegian (and foreign) students apply for admission
to college. Admission is mainly decided by the point score of the students applying to
a particular programme, which is mainly made up by the students grades. As an example, if 1,500 apply to the law programme at the University of Oslo with
230 places, then the top 15.3 % of the students in terms of points are admitted, and the admission cutpoint is the point score of this percentile.</p>
<p>Taking another view, if we consider the population of potential students applying to the programme,
the students with a point score above the <span class="math inline">\(100\times(1500-230)/1500\)</span> or 84.7-th percentile are admitted. After the application season is over, the admission cutpoints for each programme is published, in addition to the total number of applicants and available positions.</p>
<p>The offers from the application round is available the 15th of July every year, and most college programmes commence about a month later, in mid August. This means that tens of thousands of new students have to move cross country, find housing and get settled in a mere 30 days, amidst other holiday plans.</p>
<p>What if we could predict the admission cutpoints for all programmes earlier in the spring? In this post I will explore a simple model for the admission cutpoint, and use <em>Approximate Bayesian Computing</em> (ABC) to estimate the model parameters.</p>
<div id="a-model-for-the-admission-cutpoint" class="section level1">
<h1>A model for the admission cutpoint</h1>
<p>Given the three datapoints</p>
<ul>
<li><em>Admission cutpoint</em> <span class="math inline">\(c_t\)</span></li>
<li><em>Applicants</em> <span class="math inline">\(a_t\)</span></li>
<li><em>Available seats</em> <span class="math inline">\(s_t\)</span></li>
</ul>
<p>for years <span class="math inline">\(t = 1, ..., T\)</span>, how we can model the admission cutpoint given
the number of applicants <span class="math inline">\(a_t\)</span> and available seats <span class="math inline">\(s_t\)</span> in a given year? Usually, thinking about the underlying process generating the data is a fruitful approach.</p>
<p>In this case, we can assume that we have some population of applicants for a particular programme in year <span class="math inline">\(t\)</span>, parameterised by <span class="math inline">\(\theta_t\)</span>. Then, the admission cutpoint is generated by drawing <span class="math inline">\(a_t\)</span> applicants from this distribution, and calculating <span class="math inline">\(c_t = (a_t - s_t)/a_t\)</span>-quantile. In a Bayesian setting, we can then estimate the <span class="math inline">\(\theta\)</span>-parameters by proposing a prior <span class="math inline">\(p(\theta)\)</span> and calculating the posterior distribution <span class="math display">\[p(\theta|C) = \frac{p(C|\theta)p(\theta)}{p(C)}.\]</span></p>
<p>While the probabilistic process producing the admission cutpoints <span class="math inline">\(C\)</span> is simple, actually formulating the likelihood of the parameters <span class="math inline">\(\theta_t\)</span> given the cutpoints <span class="math inline">\(c_1, ..., c_T\)</span> is rather difficult (at least for me, it seems like this needs marginalizing out all individual applicants scores…)</p>
<p>However, <em>Approximate Bayesian Computation (ABC)</em> is a method for estimating the posterior distribution in settings where the likelihood is difficult/impossible to formulate or evaluation is computationally intensive, while simulation is doable. Before we apply this method to the admission cutpoint data, I will briefly describe the main idea.</p>
</div>
<div id="approximate-bayesian-computation" class="section level1">
<h1>Approximate Bayesian Computation</h1>
<p>Assuming we have a parameter vector <span class="math inline">\(\theta\)</span>, a prior <span class="math inline">\(p(\theta)\)</span>, data <span class="math inline">\(D\)</span> and a likelihood <span class="math inline">\(p(D|\theta)\)</span> we can simulate from, we can approximate <span class="math inline">\(p(\theta|D)\)</span> by generating samples <span class="math inline">\(\theta_i\)</span> from it in the following fashion:</p>
<ol style="list-style-type: decimal">
<li>Simulate <span class="math inline">\(\theta_i\)</span> from <span class="math inline">\(p(\theta)\)</span>.</li>
<li>Simulate <span class="math inline">\(D_i\)</span> from <span class="math inline">\(p(D_i|\theta_i)\)</span>.</li>
<li>Accept <span class="math inline">\(\theta_i\)</span> if <span class="math inline">\(\rho(D, D_i)\leq \epsilon\)</span>.</li>
</ol>
<p>In other words, <span class="math inline">\(\theta_i\)</span> is considered a parameter vector sampled from the posterior if the corresponding generated dataset <span class="math inline">\(D_i\)</span> is close to <span class="math inline">\(D\)</span> in some sense. The method is exact if <span class="math inline">\(\epsilon = 0\)</span>.</p>
<p>A proof for this is simple in a setting where <span class="math inline">\(p(D) &gt; 0\)</span>. Let <span class="math inline">\(\mathbb{I}_{D_i = D}\)</span>, and consider the joint distribution of <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(\mathbb{I}_{D_i = D}\)</span> generated by the algorithm:
<span class="math display">\[p(\theta_i)\times p(D_i = D |\theta_i)^{\mathbb{I}_{D_i = D}}p(D_i \neq D|\theta_i)^{\mathbb{I}_{D_i \neq D}}.\]</span> It is clear that the distribution of <span class="math inline">\(\theta_i | \mathbb{I}_{D_i = D} = 1\)</span> is proportional to <span class="math inline">\(p(\theta_i)\times p(D_i = D|\theta_i)\)</span>.</p>
<div id="choice-of-discrepancy-measure-rho" class="section level2">
<h2>Choice of discrepancy measure <span class="math inline">\(\rho\)</span></h2>
<p>The main headache in the basic ABC algorithm is the choice of discrepancy measure <span class="math inline">\(\rho\)</span> and threshold <span class="math inline">\(\epsilon\)</span>. The most naive <span class="math inline">\(\rho\)</span> is a distance measure between the actual datasets <span class="math inline">\(D_i\)</span> and <span class="math inline">\(D\)</span>, but the curse of dimensionality quickly ensures that the probability of accepting <span class="math inline">\(\theta_i\)</span> approaches 0.</p>
<p>Usually, the discrepancy measure is reduced to measuring the difference between summary statistics, e.g <span class="math inline">\(\theta_i\)</span> is accepted if <span class="math inline">\(\rho(S(D_i), S(D)) \leq \epsilon\)</span>. If a sufficient summary statistic is available for a particular setting, the ABC algorithm is exact when <span class="math inline">\(\epsilon\)</span> approaches 0.</p>
<p>In general, one must accept some information loss when using summary statistics, but the approximation of the posterior distribution is usually acceptable.</p>
<p>In our setting, a particular choice of summary statistic is very natural. Our observed data is actually just the empirical <span class="math inline">\(100 \times (a_t - s_t)/a_t\)</span>-percentile of the population of applicants. Thus, a natural discrepancy measure is the squared distance between the observed empirical <span class="math inline">\(100 \times (a_t - s_t)/a_t\)</span>-percentile and the corresponding generated percentile.</p>
</div>
</div>
<div id="estimating-the-admission-cutpoint" class="section level1">
<h1>Estimating the admission cutpoint</h1>
<p>Back to the question on estimating programme admission cutpoints, we need to choose a distribution for the point scores of the applicants. Often, students’ grades are approximately normally distributed in a discrete setting. Further, the point scores is a sum of many independent grades, which should also then be approximately normal.</p>
<p>However, the choice of normally distributed point scores introduces another estimation problem. If we believe that the mean and variance is slowly changing between consecutive years, two parameters must be estimated by observing only one empirical percentile of the distribution. Since percentiles in the normal distribution is a function of both the mean and the variance, there is an infinite combination of <span class="math inline">\((\mu_t, \sigma_t^2)\)</span>-parameters corresponding to one particular percentile.</p>
<p>The solution is to use prior information. For example, we can assume that the variance is approximately equal for all years. This might be reasonable, and this will probably lead to identifiable <span class="math inline">\(\mu_t\)</span>-parameters.</p>
<pre class="r"><code>require(tidyverse)
law &lt;- read_csv(&quot;law.csv&quot;) %&gt;% rename(c_t = Poenggrense,
                                      a_t = Søkere,
                                      s_t = Plasser) %&gt;% mutate(y_t = 2009:2020)
ggplot(law, aes(x = y_t, y = c_t)) +
  geom_line()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>prior_mu_1_mu &lt;- 40 #Prior mean for the average point score in 2009
prior_mu_1_sig &lt;- 5 #Prior std.dev for the average point score in 2009
delta_mu_sig &lt;- 1 #Std.dev of the mu_t+1 - mu_t random walk prior

sig_alpha &lt;- 5
sig_beta &lt;- 1 #scale 

iterations &lt;- 1e4
eps &lt;- 10

n_yrs &lt;- 12
a_t &lt;- law %&gt;% select(a_t) %&gt;% pull()
s_t &lt;- law %&gt;% select(s_t) %&gt;% pull()
c_t &lt;- law %&gt;% select(c_t) %&gt;% pull()
q_t &lt;- (a_t - s_t)/a_t

mu &lt;- rep(0, 11)
sim_c_t &lt;- mu
sigma &lt;- 0

acc_mu &lt;- matrix(NA, nrow = 1, ncol = n_yrs)
acc_sigma &lt;- c(NA)

for(i in 1:iterations){
  mu[1] &lt;- rnorm(1, prior_mu_1_mu, prior_mu_1_sig)
  mu[2:n_yrs] &lt;- mu[1] + cumsum(rnorm(n_yrs - 1, 0, delta_mu_sig))
  sigma &lt;- rgamma(1, shape = 5, scale = 1)
  for(t in 1:n_yrs){
    y_t &lt;- rnorm(a_t[t], mu[t], sigma)
    sim_c_t[t] &lt;- quantile(y_t, q_t[t])
  }
  
  rho &lt;- mean((sim_c_t - c_t)^2)
  if(rho &lt; eps){
    acc_mu &lt;- rbind(acc_mu, mu)
    acc_sigma &lt;- c(acc_sigma, sigma)
  }
}

post_mu &lt;- colMeans(acc_mu, na.rm = TRUE)
post_sigma &lt;- mean(acc_sigma, na.rm = TRUE)

plot(2009:2020, post_mu)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
</div>
