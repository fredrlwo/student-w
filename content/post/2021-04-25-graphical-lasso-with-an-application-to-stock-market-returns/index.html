---
title: Graphical lasso with an application to stock market returns
author: Fredrik Wollbraaten
date: '2021-04-25'
slug: graphical-lasso-with-an-application-to-stock-market-returns
categories:
  - blog
tags:
  - Applications
  - Statistical Methods
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#graphical-models">Graphical Models</a><ul>
<li><a href="#sparse-regression-models">Sparse regression models</a></li>
<li><a href="#partial-correlation">Partial correlation</a></li>
<li><a href="#partial-correlation-whats-the-use">Partial correlation… Whats the use?</a></li>
<li><a href="#last-ingredient---the-pairwise-markov-property">Last ingredient - the pairwise Markov Property</a></li>
</ul></li>
<li><a href="#the-graphical-lasso">The graphical lasso</a></li>
<li><a href="#application-to-the-oslo-stock-exchange">Application to the Oslo Stock Exchange</a></li>
</ul>
</div>

<pre class="r"><code>require(ggpubr)
require(tidyverse)
require(latex2exp)
require(stringr)
require(lubridate)
require(ggrepel)
require(glasso)
require(igraph)</code></pre>
<div id="graphical-models" class="section level2">
<h2>Graphical Models</h2>
<p>Graphical models are not models in the standard usage of the word, but a
framework for separating the data model and the dependence structure among <span class="math inline">\(p\)</span>
random variables. The dependence structure in our data is important because it is
not only the number of observations in our datasets that gets larger and
larger every year, but also the number of random variables. Further, the dependence
among these variables are by implication also more complex because the number of
ways the variables can depend on each other grows.</p>
<p>In a setting where we know nothing about the random variables except that they are continuous
and approximately Gaussian, a naive approach is to assume multivariate normality, and estimate a
covariance matrix consisting of <span class="math inline">\(\mathcal{O}(p^2)\)</span> parameters. This becomes unstable for relatively modest <span class="math inline">\(p\)</span>. Thus, we require sparsity in the covariance matrix to obtain stable estimates. Additionally, we
usually want sparsity to be able to interpret our model. But, how do we simultaneously obtain
an estimate of the covariance matrix while enforcing sparsity?</p>
<div id="sparse-regression-models" class="section level3">
<h3>Sparse regression models</h3>
<p>Consider a regression setting where we have a column vector of observations <span class="math inline">\(Y\)</span>(<span class="math inline">\(1\times n\)</span>) and covariates
<span class="math inline">\(X\)</span>(<span class="math inline">\(n\times p\)</span>), and assume that <span class="math inline">\(Y = X\beta + \epsilon\)</span> is a reasonable model. We can cast this model in the graphical model framework:</p>
<p><img src="/images/regression_model_graph.png" style="display: block; margin: auto;" /></p>
<p>In a (directed) graphical model, the dependence structure is encoded by (directed) edges. In this case,
the arrows imply that <span class="math inline">\(Y\)</span> depends on all the covariates. To build up the intuition for the graphical
lasso, we can think of how we can enforce sparsity in this simpler setting. By removing some of these edges, the result is that the corresponding <span class="math inline">\(X\)</span> does not influence <span class="math inline">\(Y\)</span>. This is exactly what the classical
lasso regression by Tibshirani (1996), by including a penalty on the <span class="math inline">\(\beta\)</span>-parameters!</p>
<p>The famous lasso model can be formulated as <span class="math display">\[\mathrm{argmin}_{\beta} (Y - X\beta)^t(Y - X\beta) + \lambda |\beta|,\]</span> and since this penalty leads to some zeroes in our <span class="math inline">\(\beta\)</span>-parameter vector, this
can be interpreted as the corresponding variables having no edge into <span class="math inline">\(Y\)</span>, thus introducing sparsity in the graph.</p>
<p>Regression models are a bit special as graphical models, in that we have a very strong assumption on the graphical structure (the edges) and that we only care about the conditional distribution of <span class="math inline">\(p(Y|X)\)</span>, not the joint distribution <span class="math inline">\(P(Y, X)\)</span>. Thus, to generalize the estimation of sparse graphs in a full joint
distribution of <span class="math inline">\(p\)</span> variables <span class="math inline">\(X_1, ..., X_p\)</span>, we need a parameterisation where parameters
estimated to be 0 corresponds to reducing the number of dependencies in a graph.</p>
</div>
<div id="partial-correlation" class="section level3">
<h3>Partial correlation</h3>
<p>It turns out that the idea of partial correlation is very helpful. In <span class="math inline">\(\mathbb{R}^3\)</span>, the partial
correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> given <span class="math inline">\(X_3\)</span>, is the correlation between <span class="math inline">\(e_{x_1}\)</span> and <span class="math inline">\(e_{x_2}\)</span> - the residuals when regressing <span class="math inline">\(X_1\)</span> on <span class="math inline">\(X_3\)</span> and <span class="math inline">\(X_2\)</span> on <span class="math inline">\(X_3\)</span>. Lets do a small simulation
to get a feeling of the idea.</p>
<pre class="r"><code>set.seed(2021)
#Covariance matrix
cov &lt;- matrix(c(1, 0.2, 0.8,
                0.2, 1, 0.1,
                0.8, 0.1, 1), byrow = T, ncol = 3)
#Simulate x_1, x_2, x_3 from multivariate normal
X &lt;- mvtnorm::rmvnorm(1e3, mean = rep(0, 3), sigma = cov)
colnames(X) &lt;- c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)
X &lt;- data.frame(X)</code></pre>
<p>We regress <span class="math inline">\(X_1\)</span> on <span class="math inline">\(X_3\)</span> and <span class="math inline">\(X_2\)</span> on <span class="math inline">\(X_3\)</span>, and then plot the scaled (such that correlation = slope) residuals <span class="math inline">\(e_{x_1}/\mathrm{sd}(e_{x_1})\)</span> against <span class="math inline">\(e_{x_2}/\mathrm{sd}(e_{x_2})\)</span> with the accompanying linear model:</p>
<pre class="r"><code>lm_1 &lt;- lm(x1 ~ x3, data = X)
e_x1 &lt;- residuals(lm_1)
lm_2 &lt;- lm(x2 ~ x3, data = X)
e_x2 &lt;- residuals(lm_2)

residuals.df &lt;- tibble(e_x1 = e_x1, e_x2 = e_x2) %&gt;% mutate(e_x1 = e_x1/sd(e_x1), e_x2 = e_x2/sd(e_x2))
ggplot(data = residuals.df, aes(x = e_x1, y = e_x2)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, ) +
  stat_regline_equation(label.y = 3, aes(label = ..eq.label..)) +
  xlab(TeX(&quot;$e_{x_1}$&quot;)) +
  ylab(TeX(&quot;$e_{x_2}$&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We obtain a partial correlation estimate of <span class="math inline">\(\hat\rho_{x_1 x_2 \cdot x_3} = 0.22\)</span>. With three variables
the partial correlation is easily available as <span class="math display">\[\rho_{x_1 x_2 \cdot x_3} = \frac{\rho_{x_1 x_2} - \rho_{x_1 x_3}\rho_{x_2 x_3}}{\sqrt{1 - \rho_{x_1 x_3}^2} \sqrt{1 - \rho_{x_2 x_3}^3}}.\]</span> We calculate using the known covariance matrix, which is equal to the correlation matrix due to the unit standard deviations:</p>
<pre class="r"><code>round((cov[1, 2] - cov[1, 3]*cov[2, 3])/(sqrt(1 - cov[1, 3]^2)*sqrt(1 - cov[2, 3]^2)), 1)</code></pre>
<pre><code>## [1] 0.2</code></pre>
<p>In statistics, <span class="math inline">\(0.24 \approx 0.20\)</span> so we seem to have gotten it right.</p>
</div>
<div id="partial-correlation-whats-the-use" class="section level3">
<h3>Partial correlation… Whats the use?</h3>
<p>To get an idea for why partial correlation can be useful in estimating sparse graphs, we can
try to make sense of what a partial correlation close to 0 means. A partial correlation close to 0 implies that when we account for all other (relevant) variables <span class="math inline">\(Z\)</span>, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is uncorrelated.</p>
<p>A stock market analogy might be two a priori unrelated companies, say FaceBook (<span class="math inline">\(X_1\)</span>) and BP (<span class="math inline">\(X_2\)</span>), whose returns are correlated when we do not account for ‘the market’(<span class="math inline">\(X_3\)</span>), but uncorrelated when we account for the market (have not actually checked, though).</p>
<p>When a partial correlation is close to 1, there is some residual relationship between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> even if we account for <span class="math inline">\(X_3\)</span>. This implies that there is a direct relationship between these variables.</p>
<p>Another stock market analogy may be the daily returns of two oil companies Aker BP (<span class="math inline">\(X_1\)</span>) and Equinor (<span class="math inline">\(X_2\)</span>), which of course is correlated without accounting for the market, but also when we account for the market. The idea here is that there is some ‘oil market’ specific driver which is still present after accounting for the general market.</p>
<p>As usual, the normal distribution has magical properties. In this case, the magical property is the equivalence between 0 partial correlation and conditional independence;
<span class="math display">\[
\rho_{x y \cdot z} = 0 \iff X \perp Y | Z \quad\text{(1)}
\]</span></p>
<p>where <span class="math inline">\(Z = \mathcal{X}\setminus\{X, Y\}\)</span> and <span class="math inline">\(\mathcal{X} = X \cup Y \cup Z\)</span>.
In words, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are conditionally independent given all other variables if and only if the partial correlation given all other variables is 0. This is trivial to show in the <span class="math inline">\(p = 3\)</span> setting. Assume
<span class="math display">\[
\begin{pmatrix}
X_1 \\
X_2 \\
X_3
\end{pmatrix}
\sim
\mathcal{N}
\begin{pmatrix}
\mu_1 \\
\mu_2 \\
\mu_3
\end{pmatrix} 
\begin{pmatrix}
\sigma_1^2 &amp; \rho_{12}\sigma_1 \sigma_2 &amp; \rho_{13}\sigma_1 \sigma_3 \\
\rho_{12}\sigma_1 \sigma_2 &amp; \sigma_2^2 &amp; \rho_{23}\sigma_2 \sigma_3 \\
\rho_{13}\sigma_1 \sigma_3 &amp; \rho_{23}\sigma_2 \sigma_3 &amp; \sigma_3^3
\end{pmatrix}
\]</span>
and calculate the conditional distribution <span class="math inline">\((X_1, X_2)^T|X_3 = x_3\)</span>. Conditional distributions are simple in the multivariate normal setting, see <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions">Wikipedia</a>. By carrying out the calculations we obtain the conditional distribution <span class="math inline">\((X_1, X_2)^T|X_3 = x_3\)</span>, which is
again normal, with a certain mean (again, see Wikipedia), but more importantly the conditional covariance between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is <span class="math display">\[\sigma_1 \sigma_2(\rho_{12} - \rho_{13}\rho_{23}).\]</span></p>
<p>Further, the partial correlation between two variables is a function of <span class="math display">\[\rho_{12} - \rho_{13}\rho_{23}\]</span> when <span class="math inline">\(X_3\)</span> is a single variable. Then, in the <span class="math inline">\(p = 3\)</span> setting, we see that the covariance between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> given <span class="math inline">\(X_3 = x_3\)</span> is 0 when the partial correlation is 0 and vice versa, showing the equivalence (1). Higher dimensions require more tedious matrix calculations.</p>
</div>
<div id="last-ingredient---the-pairwise-markov-property" class="section level3">
<h3>Last ingredient - the pairwise Markov Property</h3>
<p>We need one more ingredient to connect the concepts of sparse estimation of graphical models and 0 partial correlation. In the previous section, we showed that when the partial correlation between two variables given the rest is 0, we have conditional independence of the two variables given the rest. In undirected graphical models, conditional independence of two variables given the rest is related to the <em>pairwise Markov property</em>:
<span class="math display">\[
\text{An edge between X and Y is absent } \iff X\perp Y | Z
\]</span></p>
<p>The main point is that if two variables are conditionally independent given all other variables, there should be no edge between them in an undirected graphical model. This gives us a recipe for constructing <em>a</em> graphical model for <span class="math inline">\(X_1, ..., X_p\)</span>, by constructing a sparse estimate of the inverse covariance matrix and adding an edge between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> if the corresponding partial correlation is non-zero.</p>
</div>
</div>
<div id="the-graphical-lasso" class="section level2">
<h2>The graphical lasso</h2>
<p>When we know the relationship between the multivariate normal distribution, partial correlations, the pairwise Markov property and undirected graphical models, the graphical lasso is rather simple.</p>
<p>The method assumes that the data are multivariate normal, and maximize the log likelihood with a lasso penalty on the partial correlations. The partial correlations actually uniquely identifies the covariance matrix; they are proportional to the entries in the inverse covariance matrix, also called the precision matrix. The partial correlations which are estimated to be 0 corresponds to having no edge between the variables in the undirected graphical model - thus achieving sparsity. This optimization problem is not trivial, so of equal importance is the algorithm they produce which is orders of magnitude faster than similar approaches.</p>
<p>We will include some details on the algorithm. The penalized log likelihood of <span class="math inline">\(n\)</span> assumed normal vectors <span class="math inline">\(X_i\)</span> can be written as <span class="math display">\[\ell(\Theta) = \log \det{\Theta} - \mathrm{Tr}(S \Theta) - \rho ||\Theta||_1\]</span> where <span class="math inline">\(\Theta = \Sigma^{-1}\)</span>, i.e the precision matrix, <span class="math inline">\(S\)</span> being the empirical covariance matrix and <span class="math inline">\(||\cdot||_1\)</span> is an <span class="math inline">\(\ell_1\)</span>-norm. Remember that our goal is sparse estimation of <span class="math inline">\(\Theta\)</span>. If we let <span class="math inline">\(W\)</span> be our current estimate of <span class="math inline">\(\Theta\)</span>, we can write <span class="math inline">\(W\)</span> and <span class="math inline">\(S\)</span> in a partitioned format
<span class="math display">\[W = 
\begin{pmatrix}
W_{11} &amp; w_{12}\\
w_{12}^T &amp; w_{22}
\end{pmatrix},\quad S = 
\begin{pmatrix}
S_{11} &amp; s_{12}\\
s_{12}^T &amp; s_{22}
\end{pmatrix}.
\]</span>
Here, <span class="math inline">\(W_{11}\)</span> and <span class="math inline">\(S_{11}\)</span> is of size <span class="math inline">\((p-1) \times (p - 1)\)</span>, <span class="math inline">\(w_{12}\)</span> and <span class="math inline">\(s_{12}\)</span> is of size <span class="math inline">\((p-1)\times 1\)</span>, and finally <span class="math inline">\(w_{22}\)</span> and <span class="math inline">\(s_{22}\)</span> is <span class="math inline">\(1 \times 1\)</span>. The idea here is that given <span class="math inline">\(W_{11}\)</span>, the solution to <span class="math inline">\(w_{12}\)</span> is found by first solving:
<span class="math display">\[\min_{\beta} \frac{1}{2}||W_{11}^{1/2}\beta - b||^2 + \rho||\beta||_1,\]</span> where <span class="math inline">\(b = W_{11}^{-1/2}s_{12}.\)</span> Then <span class="math inline">\(w_{12} = W_{11}\beta\)</span>. This is another reason for discussing lasso regression earlier; this looks very similar to a lasso problem! The complete algorithm is simply choosing a starting <span class="math inline">\(W\)</span>, and cycle through and update <span class="math inline">\(w_{12}\)</span> by the lasso problem until all parameters of <span class="math inline">\(W\)</span> converge.</p>
</div>
<div id="application-to-the-oslo-stock-exchange" class="section level1">
<h1>Application to the Oslo Stock Exchange</h1>
<p>An interesting application is to estimate a graphical structure over the daily returns of the stocks comprising the Oslo Stock Exchange. In other words, we want to investigate which companies have correlated movements. Correlation analysis of stock movements is probably among the oldest quantitative investment strategies, and while you may not become the new Jim Simons by applying the graphical lasso instead of the usual correlation matrix, there are benefits in taking this approach. Perhaps the main benefit, which we have discussed earlier, is that the estimated correlation takes into account all other variables.</p>
<p>We have available price data on a selection of the companies listed on Oslo Stock Exchange:</p>
<pre class="r"><code>#Read in the stock prices and calculate the daily change &amp; returns, and remove NA
#values, which are primarily the prices for companies on dates before 
#they were listed on Oslo Stock Exchange. 
stocks &lt;- read_csv(&quot;obx.csv&quot;) %&gt;% mutate(Date = dmy(Date)) %&gt;% 
  mutate_if(is.character, as.double) %&gt;%  
  pivot_longer(cols = !Date, names_to = &quot;Comp&quot;, values_to = &quot;Price&quot;) %&gt;%
  mutate(Price = Price/100) %&gt;% group_by(Comp) %&gt;% arrange(Date) %&gt;% mutate(Change = Price - lag(Price)) %&gt;%   mutate(Return = Change/lag(Price)) %&gt;% ungroup() %&gt;% filter(!is.na(Price))

#Plotting the price movements..
stocks_label &lt;- stocks %&gt;% group_by(Comp) %&gt;% arrange(Date) %&gt;% 
  summarise(x = last(Date), y = last(Price), Comp = last(Comp))

ggplot(stocks, aes(x = Date, y = Price)) +
  geom_line(aes(color = Comp))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>stocks_standardized &lt;- stocks %&gt;% group_by(Comp) %&gt;% mutate(Return = (Return - mean(Return, na.rm = TRUE))/sd(Return, na.rm = TRUE))

stocks_matrix &lt;- stocks_standardized %&gt;% pivot_wider(id_cols = Date, names_from = Comp, values_from = Return) %&gt;% select(-Date) %&gt;% as.matrix()
cov_mat &lt;- cov(stocks_matrix, use = &quot;pairwise.complete.obs&quot;)</code></pre>
<p>Lets apply the graphical lasso to the mean-centered and standardized returns. The estimation is carried out by the <em>glasso</em>-package and creating the graph is done through the <em>igraph</em>-package. We choose the penalty parameter <span class="math inline">\(\rho = 0.45\)</span>, somewhat ad-hoc, in practice we would probably apply cross-validation.</p>
<pre class="r"><code>S &lt;- cov_mat #Sample covariance matrix
rho &lt;- 0.45 #Penalty on partial correlations
invcov &lt;- glasso(S, rho=rho)  
P &lt;- invcov$wi
colnames(P) &lt;- colnames(S)
rownames(P) &lt;- rownames(S)

parr.corr &lt;- matrix(nrow=nrow(P), ncol=ncol(P))
for(k in 1:nrow(parr.corr)) {
  for(j in 1:ncol(parr.corr)) {
    parr.corr[j, k] &lt;- -P[j,k]/sqrt(P[j,j]*P[k,k])
  }
}
colnames(parr.corr) &lt;- colnames(P)
rownames(parr.corr) &lt;- colnames(P)
diag(parr.corr) &lt;- 0

#Build network graph
stock_graph &lt;- graph_from_adjacency_matrix(parr.corr, mode=&quot;undirected&quot;, weighted = TRUE)

plot(stock_graph)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The graphical lasso has managed to extract two main clusters, which are the oil &amp; gas related companies, and the seafood companies, which is exactly the type of knowledge we want to extract with this method. Further, both Storebrand and DNB is included in the oil &amp; gas cluster, which is perhaps a bit surprising. One explanation might be that large and stable companies are counter-cyclical to the boom-and-bust oil &amp; gas sector, meaning that money tends to move to safe harbors in volatile times. If this is the correct explanation, the movements of Storebrand and DNB should be negatively correlated to the general oil &amp; gas markets after controlling for the general market movement.</p>
</div>
